<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html lang="en">
  <head>
    <meta charset="utf-8">
    <title>Mia: Stereo SLAM</title>
    <link href="css/bootstrap.min.css" rel="stylesheet"/>
  </head>
  <body>
    <script type="text/x-creole-wiki" id="report">
== Introduction
Mia is a triangulation platform. 
The ultimate goal of our system
is to do real time Simultaneous Localization And Mapping (SLAM) with this system in an unknown area.
The achieve this goal, we require another devices to detect the environment. The common ones are
specific sensors, which can measure the distances between objects precisely.
To incoporate what we learned in the semester in the class, we adopt a ubiquitous 
device -- camera, for detection.

Our system employs two cameras to build a dual-camear system, and then projects
to result to the third camera to double check our the robustness of our result. 
There are some reasons to employ two cameras. It is difficult to measure the scene depth (distance) 
from the single picture taken from a single camera. Although there is one state-of-the-art paper
to study it deeply, it is more difficult and biased. As the two eyes of humans, we can measure the
distance after the camera calibration.

{{ imgs/dual.jpg | Triangulation ~||width=30%}}

Besides, on a single camera localization system, if the trajectory of the device is too smooth, it will not be able to localize itself. This is similiar to the vibration of motion that happens on human eyes.

More details can be
found in the follow sections.

== Camera to Points
In the following chapter we describe how the image is processed, and how the 3D feature is factored.

=== Hardware Hacking
The weakness of multi camera system usually comes from the time gaps between devices. To resolve it, and to provide faster and more accurate triangulation, we followed the instruction in [1] to hack the PS3Eye, which is originally used as game augment reality. We connect one FSIN with VSYN, force the A2D chip samples together. We also have corresponding software architecture to prevent loss of packets. 

At the end, we have a synchronized stereo system at 120 fps, 320x240px.

{{ img/mia_buttom.jpg | Mia buttom view~||width=40%}}

=== The Third Eye
We want to render 3D object or map on the run. However, the stereo system, which outputed with YUYV format, already occupied 26MB/s. The upper bound of USB2.0 is 35MB/s. So, if we want a nice canvas, we have to choose a compressed source of video stream.

We use Logitech C920, which has a hardware H.264 compressor. It will work well on a not so violent environment. FFMpeg is utilized to transform the V4L payload to YUV data.

=== Calibration
Calibration is the heart of stereo system. If it was bad, the traingulation could diverse. We first to the stereo calibration by chessboard, and calculation the reprojection error. Then we do this to the HD camera alone, and estimate the pose of it by prior and chessboard.

{{ img/chessboard.jpg | Chessboards ~||width=40%}}

=== From Image to 3D points
Stereo input was processed by FAST corner test and ORB feature descriptor. No pyramid is used. It is quick enough to detect double images 100fps, and generate 20 best fetures. Then we maches the left and right descriptors with hamming loss by brute force.

However, the match pair may still be wrong. We conduct a Fundmental Matrix filtering on the undistorted pair of features, which elimates all that with \(|p^TFp^\prime|\) > threshold.

Then each pair is sent to do trianglulation. Each point could be seen as a ray from the focus, and we could also find the point with minimum sum of distance to the two rays. The point is treated as the 3D location of the feature.

== Simutaneously Localization and Mapping

{{ imgs/robot.jpg | Robots ~||width=30%}}

Simultaneous localization and mapping (SLAM) is a technique often used in the robotics domain to build up a map within an 
unknown environment (without any prior knowledge), or to update a map within a known environment (with a prior knowledge 
from a given map), while at the same time keeping track of theirselves current location.

=== Extended Kalman Filter

One common general algorithm for SLAM is Extended Kalman Filter (EKF).
As the name suggests, the original Kalman Filter assumes the tranformations
are all linear. For non-linear cases, Extended Kalman Filter approximates
it by the first order approximation.

{{ imgs/ekf.jpg | EKF Framwork ~||width=30%}}

The framework of EKF consists of two major tasks, includes prediction and measurement.
In prediction stage, we need to predict the current state of the camera by a prediction
function \[f(\mathbf{x})\]. Here \[\mathbf{x}\] is the \[13 \times 1\] state vector includes 
positions 
\[\mathbf{r}\], the ratation in quaternion representation 
\[\mathbf{q}\], the velocity
\[\mathbf{v}\] and the angle velocity in euler angle representation
\[\mathbf{\omega}\].

Hence, our designed prediction function \[f(\mathbf{x})\] is 

\[ \mathbf{x} = \left( \begin{array}{c} \mathbf{r}\\ \mathbf{q}\\ \mathbf{v}\\ \mathbf{\omega}\\ \end{array} \right),  \]
\[ f\left( \begin{array}{c} \mathbf{r}\\ \mathbf{q}\\ \mathbf{v}\\ \mathbf{\omega}\\ \Delta t \end{array} \right) \] 
\[ = \left( \begin{array}{c} \mathbf{r} + (\mathbf{v}+\mathbf{V})\Delta t\\ \mathbf{q}((\mathbf{\omega}+\mathbf{\Omega})\Delta t) \times \mathbf{q}\\ \mathbf{v} + \mathbf{V}\\ \mathbf{\omega} + \mathbf{\Omega} \end{array} \right)\] \\

Here we assume our camera is with a fixed velocity in every time slot \[\Delta t\], note that the velocity can change between 
different time slots. Hence, \[\mathbf{v}\] and \[\mathbf{\omega}\] is constants with two biases.
Then the position is a linear transform related to the velocity. The non-linear one here the the rotation in 
quaternion representation. The rotation transformation is by the product of two quaternions.

The second stage is the measurement stage. After predicting the current states, we estimates the related positions of
other objects related to the current state by a function \[h(\mathbf{x}, \mathbf{y})\]. In our system, that is, the 
position in the local frame of the taken pictures. If our prediction and estimation is correct, the estimated position
should be the same as the observation. Otherwise, there is the difference between them, we then could refine and
update our states and the map based on the difference.

Then the full algorithm is

\[ A \equiv \frac{ \partial f(\mathbf{x}) }{ \partial \mathbf{x} } \]

\[ H \equiv \frac{ \partial h(\mathbf{x},\mathbf{y}) }{ \partial \mathbf{x} } \]

**Prediction:**

\[\mathbf{x} = f(\mathbf{x})\]

\[\Sigma = A\Sigma A^{T} + Q\]

**Measurement:**

\[K = \Sigma H^{T}(H\Sigma H^{T}+R)^{-1}\]

\[\mathbf{x} = \mathbf{x}+K(z-h(\mathbf{x},\mathbf{y}))\]

\[\Sigma = (I-KH)\Sigma\]

=== Derivative Derivation

The derivative here is very complex. Since we **derive the** function \[f\] and \[h\] **by ourselves**, we
also need to derive the derivative of them.

\[ \frac{\partial f(\mathbf{x}) }{ \partial \mathbf{x} } = \left( \begin{array}{cccc} I & 0 & \Delta t I & 0 \\ 0 & \frac{ \partial \mathbf{q}(\mathbf{\omega}+\mathbf{\Omega}\Delta t) \times \mathbf{q} }{ \partial \mathbf{q} }& 0 & \frac{ \partial \mathbf{q}(\mathbf{\omega}+\mathbf{\Omega}\Delta t) \times \mathbf{q} }{ \partial \mathbf{\omega} } \\ 0 & 0 & I & 0 \\ 0 & 0 & 0 & I  \end{array} \right) \]


\[ \frac{ \partial \mathbf{q}(\mathbf{\omega}+\mathbf{\Omega}\Delta t) \times \mathbf{q} }{ \partial \mathbf{q} } = \left( \begin{array}{cccc} \hat{w} & -\hat{x} & -\hat{y} & -\hat{z} \\ \hat{x} & \hat{w} & \hat{z} & -\hat{y} \\ \hat{y} & -\hat{z} & \hat{w} & \hat{x} \\ \hat{z} & \hat{y} & -\hat{x} & \hat{w}  \end{array} \right) \]

where

\[\hat{w} = cos(\phi/2)cos(\theta/2)cos(\psi/2) + sin(\phi/2)sin(\theta/2)sin(\psi/2)\]

\[\hat{x} = sin(\phi/2)cos(\theta/2)cos(\psi/2) - cos(\phi/2)sin(\theta/2)sin(\psi/2)\]

\[\hat{y} = cos(\phi/2)sin(\theta/2)cos(\psi/2) + sin(\phi/2)cos(\theta/2)sin(\psi/2)\]

\[\hat{z} = cos(\phi/2)cos(\theta/2)sin(\psi/2) - sin(\phi/2)sin(\theta/2)cos(\psi/2)\]

\[\phi = \mathbf{w}_{\phi} \times \Delta t\] 

\[\theta = \mathbf{w}_{\theta} \times \Delta t\] 

\[\psi = \mathbf{w}_{\psi} \times \Delta t\] 


\[ \frac{ \partial \mathbf{q}(\mathbf{\omega}+\mathbf{\Omega}\Delta t) \times \mathbf{q} }{ \partial \mathbf{\omega} } = \frac{\Delta t}{2} \left( \begin{array}{cccc} \mathbf{q}_{w} & -\mathbf{q}_{x} & -\mathbf{q}_{y} & -\mathbf{q}_{z} \\ \mathbf{q}_{x} & \mathbf{q}_{w} & -\mathbf{q}_{z} & \mathbf{q}_{y} \\ \mathbf{q}_{y} & \mathbf{q}_{z} & \mathbf{q}_{w} & -\mathbf{q}_{x} \\ \mathbf{q}_{z} & -\mathbf{q}_{y} & \mathbf{q}_{x} & \mathbf{q}_{w} \end{array} \right) \times\]
\[\left( \begin{array}{ccc}  -sin(\phi/2)cos(\theta/2)cos(\psi/2) + cos(\phi/2)sin(\theta/2)sin(\psi/2)  &  -cos(\phi/2)sin(\theta/2)cos(\psi/2) + sin(\phi/2)cos(\theta/2)sin(\psi/2) &  -cos(\phi/2)cos(\theta/2)sin(\psi/2) + sin(\phi/2)cos(\theta/2)cos(\psi/2)  \\ cos(\phi/2)cos(\theta/2)cos(\psi/2) + sin(\phi/2)sin(\theta/2)sin(\psi/2) & sin(\phi/2)sin(\theta/2)cos(\psi/2) - cos(\phi/2)cos(\theta/2)sin(\psi/2) & -sin(\phi/2)cos(\theta/2)sin(\psi/2) - cos(\phi/2)sin(\theta/2)cos(\psi/2) \\ -sin(\phi/2)sin(\theta/2)cos(\psi/2) + cos(\phi/2)cos(\theta/2)sin(\psi/2)  & cos(\phi/2)cos(\theta/2)cos(\psi/2) - sin(\phi/2)sin(\theta/2)sin(\psi/2)  &  -cos(\phi/2)sin(\theta/2)sin(\psi/2) + sin(\phi/2)cos(\theta/2)cos(\psi/2)  \\ -sin(\phi/2)cos(\theta/2)sin(\psi/2) - cos(\phi/2)sin(\theta/2)sin(\psi/2)  &  -cos(\phi/2)cos(\theta/2)sin(\psi/2) - sin(\phi/2)cos(\theta/2)cos(\psi/2)  & cos(\phi/2)cos(\theta/2)cos(\psi/2) + sin(\phi/2)sin(\theta/2)sin(\psi/2) \end{array} \right)  \]

=== Matching

After we recieve the feature descriptors and positions from the camera system, we need to check whether the feature is 
ever observed. We use the descriptor to do brute force matching and double check whether the position is reasonable.
If it is, we use thefeatures in the measurement stage, otherwise, we extend our map by adding these new features.

== Screenshots
{{ img/screenshot.png| Screenshot ~||width=40%}}

== Open Source
The whole source, including the report, is open sourced (by us) and you can do download it on internet.

* [[https://github.com/xflash96/mia | Mia: Stereo Vision System]]

== Reference
* [1] [[http://nuigroup.com/forums/viewthread/12445/ | PS3Eye hacks]]
* [2] [[http://www.robots.ox.ac.uk/~lav/Papers/davison_etal_pami2007/davison_etal_pami2007.pdf | MonoSLAM : Real-Time Single Camera SLAM]]

    </script>
    
    <div class="navbar navbar-fixed-top">
      <div class="navbar-inner">
        <div class="container">
          <a class="btn btn-navbar" data-toggle="collapse" data-target=".nav-collapse">
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
          </a>
          <a class="brand" href="#">Mia</a>
          <div class="nav-collapse">
            <ul class="nav">
              <li><a href="#report">Report</a></li>
            </ul>
          </div><!--/.nav-collapse -->
        </div>
      </div>
    </div>

    <div class="container">
        <div id="content">
		<div class="row" id="wiki"></div>
	</div>
        <footer>
        <p>VFX 2012 Spring Final Project</p>
        </footer>
    </div> <!-- /container -->

    <!-- Le javascripts -->
    <script src="http://cdnjs.cloudflare.com/ajax/libs/modernizr/2.5.3/modernizr.min.js"></script>
    <script src="http://ajax.googleapis.com/ajax/libs/jquery/1/jquery.min.js"></script>
    <script src="http://cdnjs.cloudflare.com/ajax/libs/underscore.js/1.3.1/underscore-min.js"></script>
    <script src="http://cdnjs.cloudflare.com/ajax/libs/backbone.js/0.9.1/backbone-min.js"></script>
    <script src="http://cdnjs.cloudflare.com/ajax/libs/json2/20110223/json2.js"></script>
    <script src="http://cdnjs.cloudflare.com/ajax/libs/handlebars.js/1.0.0.beta2/handlebars.min.js"></script>
    <script src="http://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/2.0.2/bootstrap.min.js"></script>
    <script src="js/jquery.form.min.js"></script>
    <script src="js/creole.js"></script>
    <script src="js/load-image.js"></script>
    <script src="js/bootstrap-carousel.js"></script>
    <script type="text/javascript" src="js/latexit.js"></script>
    <script type="text/javascript" src="http://cdnjs.cloudflare.com/ajax/libs/prettify/188.0.0/prettify.js"></script>
    <script type="text/javascript">
	(function(){
	    var getMarkup = function(data){
		var div = $('#wiki')[0];
       	        var creole = new Parse.Simple.Creole({
		    forIE: document.all,
	        });
		creole.parse(div, data);
		prettyPrint();
		LatexIT.render('span');
	    };
	    getMarkup($('#report').text());
	})();
    </script>
    <!-- Le debugs
    <meta http-equiv="cache-control" content="no-cache">
    -->

    <!-- Le styles -->
    <style>
      body {
        padding-top: 60px; /* 60px to make the container go all the way to the bottom of the topbar */
      }
    </style>
    <link href="css/bootstrap-responsive.css" rel="stylesheet"/>
    <link href="css/prettify.css" rel="stylesheet"/>
    <!--
    <script src="/static/js/bootstrap-transition.js"></script>
    <script src="/static/js/bootstrap-tab.js"></script>
    <script src="/static/js/bootstrap-tooltip.js"></script>
    <script src="/static/js/bootstrap-popover.js"></script>
    <script src="/static/js/bootstrap-typeahead.js"></script>
    -->
    <a href="http://github.com/xflash96/mia"><img style="position: absolute; top: 40px; right: 0; border: 0;" src="https://a248.e.akamai.net/camo.github.com/abad93f42020b733148435e2cd92ce15c542d320/687474703a2f2f73332e616d617a6f6e6177732e636f6d2f6769746875622f726962626f6e732f666f726b6d655f72696768745f677265656e5f3030373230302e706e67" alt="Fork me on GitHub"></a>

  </body>
</html>
